I"©<h3 id="1-the-lottery-ticket-hypothesis-finding-sparse-trainable-neural-networks"><strong>1. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</strong></h3>
<hr />

<p><strong>Paper Asli</strong>: https://openreview.net/pdf?id=rJl-b3RcF7</p>

<p>Paper ini mendapatkan <em>best paper award</em> pada konferensi <a href="https://iclr.cc/">ICLR 2019</a>, yang merupakan salah satu konferensi tingkat atas di bidang <em>deep learning</em>.</p>

<p><em>Deep neural networks</em> (DNN) yang memiliki performa terbaik biasanya memiliki arsitektur model yang besar dan mendalam, yang dilatih dengan sejumlah data yang masif.
Pada domain <em>computer vision</em>, beberapa arsitektur model populer seperti AlexNet memiliki jumlah parameter sebesar 61 juta dan VGG-16 memiliki jumlah parameter 138 juta.</p>

<p>Arsitektur model yang besar memiliki efek samping yaitu penambahan kompleksitas waktu dan memori baik dari fase pelatihan maupun fase inferensi.
Jika kita memiliki model DNN yang jauh lebih kecil dengan performa akurasi yang serupa dengan model yang besar, itu akan sangat banyak manfaatnya dari sisi aplikasi.
DNN ukuran yang kecil akan lebih mudah untuk diimplementasi pada perangkat <em>mobile</em> atau IoT yang notabene memiliki batasan komputasi dan memori dibandingkan komputer normal.</p>

<p>Di beberapa tahun terakhir ada berbagai percobaan untuk memangkas arsitektur model DNN. 
Sebagai contoh, (<a href="https://arxiv.org/pdf/1506.02626.pdf">Han et al. NIPS 2015</a>) membuat algoritma yang berhasil memangkas DNN hingga ~90% lebih kecil dari model aslinya tanpa mengurangi performa akurasi saat inferensi â€“ menurunkan parameter AlexNet ke 6.7 juta dan VGG-16 ke 10.3 juta.</p>

<p>Namun demikian, metode tersebut, yang selanjutnya dikenal dengan <em>deep compression</em>, sebatas memangkas model DNN besar yang sebelumnya sudah dilatih, belum menangani bagaimana memangkas DNN pada saat pelatihan.
Pemangkasan pada saat pelatihan akan secara natural meningkatkan efisiensi proses pelatihan.</p>

<p>Paper ini mengusulkan sebuah algoritma untuk memangkas arsitektur DNN dari saat proses pelatihan.
Algoritma tersebut dilatarbelakangi oleh sebuah hipotesis yang penulisnya namakan sebagai <strong>Lottery Ticket Hypothesis</strong>:
<em>Sebuah DNN yang parameternya diinisialisasi secara random memiliki sebuah sub-network yang parameternya diinisialisasi sedemikian rupa sehingga apabila sub-network tersebut dilatih, performa akurasinya akan serupa dengan DNN yang besar dengan jumlah iterasi pelatihan yang serupa.</em></p>

<p>Algoritma yang diusulkan akan menghasilkan sebuah <em>sub-network</em> yang disebut dengan <em>winning ticket</em>, yang secara umum terdiri dari 4 tahapan:</p>
<ol>
  <li>Inisialisi parameter DNN secara acak: \(f(x; \theta_0)\), dimana \(\theta_0 \sim \mathcal{D}_\theta\).</li>
  <li>Latih DNN sebanyak iterasi sejumlah \(j\), menghasilkan parameter \(\theta_j\).</li>
  <li>Pangkas \(p\%\) dari parameter \(\theta_j\) sehingga menghasilkan <em>mask</em> \(m\).</li>
  <li>Kembalikan nilai parameter yang tersisa ke nilai pada \(\theta_0\), yang menghasilkan <em>winning ticket</em> \(f(x; m \odot \theta_0)\).</li>
</ol>

<p>Berbagai hasil eksperimen yang dilaporkan pada paper tersebut menunjukkan bahwa hasilnya mendukung <em>lottery ticket hypothesis</em> dan secara konsisten mampu menghasilkan <em>sub-network</em> yang berukuran ~90% lebih kecil dibandingkan model aslinya melalui proses pelatihan pada dataset MNIST dan CIFAR10.
<!-- Manfaat utama dari algoritma ini adalah membuat proses pelatihan secara keseluruhan lebih efisien dibandingkan cara konvensional. -->
Model dengan parameter yang jauh lebih sedikit mengakibatkan proses inferensi menjadi lebih efisien.</p>
:ET